%!TEX root=paper/paper.tex
\section{Evaluation}\label{sec:clf_evaluation}

\PM{Settings}
We evaluate the following sequential selection baselines:

\begin{itemize}
\item \textbf{Static, greedy}: corresponds to best performance of a policy that does not observe feature values and selects actions greedily ($\gamma=0$).
\item \textbf{Static, non-myopic}: policy that does not observe feature values but uses the MDP machinery with $\gamma=1$ to consider future action rewards.
\item \textbf{Dynamic, greedy}: policy that observed feature values, but selects actions greedily.
\end{itemize}

Our method is the \textbf{Dynamic, non-myopic} policy: observed feature values, and full lookahead.

\PM{Metrics}
We evaluate two forms of test-time efficient performance measure: the area under the curve and the performance at max budget.
(Note that all methods are trained only for the former measure.)

\PM{Classifier}
In preliminary experiments, Logistic Regression always performed better than the Gaussian Naive Bayes classifier, and so only the former is used in the experiments below.
We evaluated classification with \textbf{Gaussian} vs. \textbf{Mean} imputation, and with different number of classifiers (1, 3, and 6) clustered by feature subsets.
We found that Gaussian imputation performed better than mean imputation, but not substantially, and at additional cost.
Although increased number of classifiers sometimes increased performance, it also made our method prone to overfitting.
In the final accounting, $K=1$ classifiers worked best on all tasks.
Results of detailed experiments on imputation and clustering are given in \autoref{sec:imputation_evaluation}.
For policy experiments, we report only the best achieved imputation method.

\PM{Details}
For details on the reinforcement learning implementation, see \autoref{sec:mdp_formulation} and \autoref{sec:det_evaluation}.
We largely rely on classifier implementations in the \texttt{scikit-learn} package \parencite{Pedregosa2011}.
With pre-computed features, our training procedure takes only a few hours on an $8$-core \emph{Xeon E5620} machine for each of the experiments below.

\subsection{Experiment: Synthetic}

\input{../eval_synthetic_fig}

\PM{Setup}
Following \cite{Xu-ICML-2013}, we first show that the policy works as advertised in a challenging synthetic example.
In $D$-dimensional space, the data has a label for each of the $2^D$ orthants, and is generated by a unit-variance Gaussian in that orthant (See top left of \autoref{fig:synthetic_setup} for the 3D case).
There are $D$ cheap features that simply return the sign of the data point's coordinate for the corresponding dimension.
For each orthant, there is also an expensive feature that returns the data point's label if the point is located in the corresponding orthant, and random noise otherwise.

\PM{Results}
The optimal policy on a new data point is to determine its orthant with cheap features, and then take the corresponding expensive action.
Note that both dynamic features and non-myopic learning are crucial to the optimal policy, which is successfully found by our approach.
\autoref{fig:synthetic_results} shows the results of this optimal policy, a random policy, and of different baselines and our method, trained given the correct minimal budget.

\subsection{Experiment: Scene recognition}

\input{../eval_scenes_fig}

\PM{Setup}
The Scene-15 dataset \parencite{Lazebnik-CVPR-2006} contains 4485 images from 15 visual scene classes.
The task is to classify images according to scene.
Following \parencite{Xiao-CVPR-2010}, we extracted 14 different visual features (GIST, HOG, TinyImages, LBP, SIFT, Line Histograms, Self-Similarity, Textons, Color Histograms, and variations).
The features vary in cost from 0.3 seconds to 8 seconds, and in single-feature accuracy from 0.32 (TinyImages) to .82 (HOG).
Separate multi-class linear SVMs were trained on each feature channel, using a random 100 positive example images per class for training.
We used the \texttt{liblinear} implementation, and K-fold cross-validated the penalty parameter $C$.
The trained SVMs were evaluated on the images not used for training, resulting in a dataset of 2238 vectors of 210 confidence values: 15 classes for each of the 14 feature channels.
This dataset was split 60-40 into training and test sets for our experiments.

\PM{Results}
\autoref{fig:clf_scenes} shows the results, including learned policy trajectories.
For all evaluated budgets, our \emph{dynamic, non-myopic} method outperforms all others on the area under the error vs. cost curve metric.
Our results on this dataset match the reported results of Active Classification \parencite{Gao-NIPS-2011} (Figure 2) and Greedy Miser \parencite{Xu-ICML-2012} (Figure 3), although both methods use an additional powerful feature channel (ObjectBank)\footnote{Detailed results for this and other experiments are on the project page (see front page for the \href{http://sergeykarayev.com/recognition-on-a-budget/}{link}).}.

\subsection{Experiment: ImageNet and maximizing specificity}

%\input{../imagenet_tree_fig}

\PM{Setup}
The full ImageNet dataset has over 10K categories and over a million images \parencite{Deng-ECCV-2010}.
% \autoref{fig:imagenet_tree} shows the hierarchical organization of the dataset.
The classes are organized in a hierarchical structure, which can be exploited for novel recognition tasks.
We evaluate on a 65-class subset introduced in ``Hedging Your Bets'' \parencite{Deng-CVPR-2012}.
In this evaluation, we consider the situation where the initial feature computation has already happened, and the task is to find a path through existing one-vs-all classifiers: features correspond to Platt-scaled SVM confidences of leaf-node classifiers (trained on SIFT-LLC features), and each has cost 1 \parencite{Deng-ECCV-2010}.
Following \parencite{Deng-CVPR-2012}, accuracy is defined on all nodes, and inner node confidences are obtained by summing the probabilities of the descendant nodes.

\PM{Results}
We combine our sequential feature selection with the ``Hedging Your Bets'' method for backing off prediction nodes using the ImageNet hierarchy to maintain guaranteed accuracy while giving maximally specific answers, given a cost budget.
The results are given in \autoref{fig:imagenet}.
As the available budget increases, the \emph{specificity} (defined by normalized information gain in the hierarchy) of our predictions also increases, while accuracy remains constant.
Visualizing this on the ILSVRC-65 hierarchy, we see that the fraction of predictions at the leaf nodes grows with available computation time.
This formulation presents a novel angle on modeling the time course of human visual perception.

\input{../eval_imagenet_fig}
